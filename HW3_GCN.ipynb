{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHoQThpObPSq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第三次作业\n",
    "\n",
    "在本次作业中，大家首先通过代码来模拟课件里描述的GCN例子的计算过程。\n",
    "然后根据计算过程，大家自己来实现一个朴素版本的GCN模型并在cora数据集上进行测试。\n",
    "最后，大家还将运行CogDL版本GCN模型的实验作为对比。\n",
    "\n",
    "本作业需要安装[CogDL](https://github.com/THUDM/cogdl)：pip install cogdl\n",
    "\n",
    "如需使用gpu版，请先安装gpu版本的[PyTorch](https://pytorch.org/get-started/locally/)，再安装cogdl。\n",
    "\n",
    "本作业由智谱GNN中心及课程团队筹备，由CogDL团队提供技术支持。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8JmSMqUEHCn7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iR3XYifiote",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**第一部分：手动模拟GCN的计算和训练过程。**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtC_P6jTh9ta",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 根据初始的邻接矩阵A得到正则化后的邻接矩阵normA。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7XKgoLPHLvv",
    "outputId": "39db09be-2b04-4b4e-ee94-c235485b1e20",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "normA= tensor([[0.2500, 0.2887, 0.2500, 0.2887],\n",
      "        [0.2887, 0.3333, 0.2887, 0.0000],\n",
      "        [0.2500, 0.2887, 0.2500, 0.2887],\n",
      "        [0.2887, 0.0000, 0.2887, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[0, 1, 1, 1], [1, 0, 1, 0], [1, 1, 0, 1], [1, 0, 1, 0]])\n",
    "A = A + torch.eye(4)\n",
    "print(\"A=\", A)\n",
    "###################\n",
    "##### 作业填空 #####\n",
    "###################\n",
    "# 计算度数矩阵D，并对A进行正则化得到normA\n",
    "D = torch.diag(A.sum(1))\n",
    "D_hat = torch.diag(1.0 / torch.sqrt(A.sum(1)))\n",
    "normA = torch.mm(torch.mm(D_hat, A), D_hat)\n",
    "print(\"normA=\", normA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-sjQ5IuiLOm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. 根据初始特征X，模型参数W1，邻接矩阵normA来计算第一层的输出H1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTrKFEnOHqKe",
    "outputId": "35b3dfb0-1bc8-47ce-fc4d-595596ac834e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1= tensor([[1.0774, 0.1830],\n",
      "        [0.7440, 0.0447],\n",
      "        [1.0774, 0.1830],\n",
      "        [1.0774, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "H0 = X = torch.FloatTensor([[1,0], [0,1], [1,0], [1,1]])\n",
    "W1 = torch.tensor([[1, -0.5], [0.5, 1]], requires_grad=True)\n",
    "###################\n",
    "##### 作业填空 #####\n",
    "###################\n",
    "# 通过normA/H0/W1计算得到H1\n",
    "H1 = F.relu(torch.mm(normA, torch.mm(H0, W1)))\n",
    "print(\"H1=\", H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgepM9zIiXf2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. 计算第二层的输出H2和最后的输出Z。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgBcMPBjKZSN",
    "outputId": "883733f7-2bcd-4a23-c7ae-ca60162bfe72",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2= tensor([[ 0.6366, -0.4800],\n",
      "        [ 0.5556, -0.3747],\n",
      "        [ 0.6366, -0.4800],\n",
      "        [ 0.5962, -0.4377]], grad_fn=<MmBackward0>)\n",
      "Z= tensor([[0.7534, 0.2466],\n",
      "        [0.7171, 0.2829],\n",
      "        [0.7534, 0.2466],\n",
      "        [0.7377, 0.2623]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W2 = torch.tensor([[0.5, -0.5], [1, 0.5]], requires_grad=True)\n",
    "###################\n",
    "##### 作业填空 #####\n",
    "###################\n",
    "# 通过normA/H1/W2计算得到H2和Z\n",
    "H2 = torch.mm(normA, torch.mm(H1, W2))\n",
    "print(\"H2=\", H2)\n",
    "Z = F.softmax(H2, dim=-1)\n",
    "print(\"Z=\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7cXQ_Lqh6D5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. 计算损失函数loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvMmgve_MIiQ",
    "outputId": "1ca06b90-a2bb-446d-d7d0-28a4c24232e3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333563685417175\n"
     ]
    }
   ],
   "source": [
    "Y = torch.LongTensor([0, 1, 0, 0])\n",
    "###################\n",
    "##### 作业填空 #####\n",
    "###################\n",
    "# 根据输出Z和标签Y来计算最后的loss\n",
    "loss = F.nll_loss(Z.log(), Y)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMW8FEFghuzk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. 通过loss进行反向传播。可以看到模型参数W1/W2的梯度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAgWX9NwNK0_",
    "outputId": "1f518dd6-9ce7-428a-d4cb-84ab6ddaf41c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.5000],\n",
      "        [ 0.5000,  1.0000]], requires_grad=True)\n",
      "tensor([[-0.0352,  0.0085],\n",
      "        [-0.0088,  0.0052]])\n",
      "tensor([[ 0.5000, -0.5000],\n",
      "        [ 1.0000,  0.5000]], requires_grad=True)\n",
      "tensor([[-0.0396,  0.0396],\n",
      "        [ 0.0018, -0.0018]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "print(W1)\n",
    "print(W1.grad)\n",
    "print(W2)\n",
    "print(W2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PmciUGbjBwQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**第二部分：使用你实现的GCN模型来运行cora数据集**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAPbTveYjONP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. 从cogdl中加载cora数据集（x表示特征，y表示标签，mask表示训练/验证/测试集的划分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61aEVO2ySq27",
    "outputId": "6da332b4-d5aa-4baf-d95e-4dd5b4b291da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(x=[2708, 1433], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_index=[2, 10556])\n"
     ]
    }
   ],
   "source": [
    "from cogdl.datasets import build_dataset_from_name\n",
    "\n",
    "dataset = build_dataset_from_name(\"cora\")\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "n = data.x.shape[0]\n",
    "edge_index = torch.stack(data.edge_index)\n",
    "A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (n, n)).to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcRyHUuIjhfT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. 使用你实现的GCN模型进行训练（在GCN模型的forward中填入你在第一部分中写的代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pr5lZC2eWCYm",
    "outputId": "c42b59ef-9637-4bac-e1f1-79a3f9f6228a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Train Loss: 0.0112, Val Loss: 0.7280, Val Acc: 0.7720: 100%|██████████| 100/100 [02:32<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc 0.774\n",
      "Test Acc 0.802\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_true = y_true.squeeze().long()\n",
    "    preds = y_pred.max(1)[1].type_as(y_true)\n",
    "    correct = preds.eq(y_true).double()\n",
    "    correct = correct.sum().item()\n",
    "    return correct / len(y_true)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats,\n",
    "        hidden_size,\n",
    "        out_feats,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "        self.out_feats = out_feats\n",
    "        self.W1 = nn.Parameter(torch.FloatTensor(in_feats, hidden_size))\n",
    "        self.W2 = nn.Parameter(torch.FloatTensor(hidden_size, out_feats))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.out_feats)\n",
    "        torch.nn.init.uniform_(self.W1, -stdv, stdv)\n",
    "        torch.nn.init.uniform_(self.W2, -stdv, stdv)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        n = X.shape[0]\n",
    "        A = A + torch.eye(n, device=X.device)\n",
    "        ###################\n",
    "        ##### 作业填空 #####\n",
    "        ###################\n",
    "        # 依次计算normA/H1/H2，然后返回H2。注意：此处不需要计算Z，因为通常直接根据H2和Y来计算loss。\n",
    "        # 注意使用self.W1/W2来调用模型参数。\n",
    "        D_hat = torch.diag(1.0 / torch.sqrt(A.sum(1)))\n",
    "        normA = torch.mm(torch.mm(D_hat, A), D_hat)\n",
    "        H1 = F.relu(torch.mm(normA, torch.mm(X, self.W1)))\n",
    "        H2 = torch.mm(normA, torch.mm(H1, self.W2))\n",
    "\n",
    "        return H2\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "model = GCN(data.x.shape[1], hidden_size, data.y.max() + 1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    A = A.to(device)\n",
    "    data.apply(lambda x: x.to(device))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epoch_iter = tqdm(range(100), position=0, leave=True)\n",
    "best_model = None\n",
    "best_loss = 1e8\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(A, data.x)\n",
    "    loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(A, data.x)\n",
    "        val_loss = F.cross_entropy(logits[data.val_mask], data.y[data.val_mask]).item()\n",
    "        val_acc = accuracy(logits[data.val_mask], data.y[data.val_mask])\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    epoch_iter.set_description(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = best_model(A, data.x)\n",
    "    val_acc = accuracy(logits[data.val_mask], data.y[data.val_mask])\n",
    "    test_acc = accuracy(logits[data.test_mask], data.y[data.test_mask])\n",
    "print(\"Val Acc\", val_acc)\n",
    "print(\"Test Acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uG_u8LIdkQh_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. 调用cogdl的GCN模型来运行cora数据集，观察两者的区别并思考（主要是训练时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOdtm6g_HgRx",
    "outputId": "a4070f98-de47-49c0-cbfd-82b093553ad4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 92231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100, train_loss:  0.0150, val_acc:  0.7800: 100%|██████████| 100/100 [00:04<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 34-th model to ./checkpoints/model.pt ...\n",
      "Loading model from ./checkpoints/model.pt ...\n",
      "{'test_acc': 0.814, 'val_acc': 0.794}\n"
     ]
    }
   ],
   "source": [
    "from cogdl.utils import set_random_seed\n",
    "from cogdl.models.nn import GCN\n",
    "from cogdl.trainer import Trainer\n",
    "from cogdl.wrappers import fetch_model_wrapper, fetch_data_wrapper\n",
    "\n",
    "set_random_seed(100)\n",
    "model = GCN(\n",
    "    in_feats=data.num_features,\n",
    "    hidden_size=64,\n",
    "    out_feats=data.num_classes,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    device_ids = [0]\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    device_ids = None\n",
    "mw_class = fetch_model_wrapper(\"node_classification_mw\")\n",
    "dw_class = fetch_data_wrapper(\"node_classification_dw\")\n",
    "optimizer_cfg = dict(lr=0.01, weight_decay=0)\n",
    "model_wrapper = mw_class(model, optimizer_cfg)\n",
    "dataset_wrapper = dw_class(dataset)\n",
    "trainer = Trainer(epochs=100,\n",
    "                  early_stopping=False,\n",
    "                  cpu=device==\"cpu\",\n",
    "                  device_ids=device_ids)\n",
    "ret = trainer.run(model_wrapper, dataset_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_MIqEgvk9T1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "write-your-first-gcn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('py3.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7703d8cc28a94efd96474faf641471b2ac55415e3c97cc8df6cd7b51cf78a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}